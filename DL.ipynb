{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialect Classification Using LSTM and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train and evaluate LSTM and RNN models for dialect classification using Arabic text data. We will preprocess the text data, convert it into numerical sequences, and then use these sequences to train our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "First, we import the necessary libraries for data manipulation and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data\n",
    "Load the cleaned dataset and display the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الجنوبمطفي لانه ناس باحزابنا شركاء مافيات المو...</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>والدتكن كمان بتقلكن ساعدوني بالتعزيل وبطس الما...</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ماعمري ماجلبت مللي كنت صغيره</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>الفاصله متفق معاك الفاصله اشريف اليوفي بكل حيا...</td>\n",
       "      <td>LY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>عجبني بزاااف كنشوفوو اوزاان ثقيله ابطاال محفل ...</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text dialect\n",
       "0  الجنوبمطفي لانه ناس باحزابنا شركاء مافيات المو...      LB\n",
       "1  والدتكن كمان بتقلكن ساعدوني بالتعزيل وبطس الما...      LB\n",
       "2                       ماعمري ماجلبت مللي كنت صغيره      MA\n",
       "3  الفاصله متفق معاك الفاصله اشريف اليوفي بكل حيا...      LY\n",
       "4  عجبني بزاااف كنشوفوو اوزاان ثقيله ابطاال محفل ...      MA"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Determine Maximum Text Length\n",
    "Calculate the maximum length of the text in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefit\n",
    "Knowing the maximum length helps in setting the appropriate input length for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "#getting max length in text column\n",
    "max_length = df['text'].apply(lambda x: len(x.split())).max()\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocess Data\n",
    "Convert the text data into numerical sequences and pad them to ensure uniform length. Also, encode the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why?\n",
    "Text to Numerical Sequences: To convert text data into a format suitable for model training.\n",
    "Padding Sequences: To ensure that all sequences have the same length.\n",
    "Label Encoding: To convert categorical labels into numerical format.\n",
    "## Benefit\n",
    "Converting text to numerical sequences and padding them ensures uniform input dimensions, which is essential for training neural networks. Encoding labels makes them suitable for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into input sequences and labels\n",
    "max_len = 61  # Maximum sequence length\n",
    "texts = df['text'].values\n",
    "labels = df['dialect'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Arabic text to numerical sequences\n",
    "num_sequences = [[ord(char) for char in text] for text in texts]\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "padded_sequences = pad_sequences(num_sequences, maxlen=max_len)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_mapping = {label: idx for idx, label in enumerate(set(labels))}\n",
    "encoded_labels = [label_mapping[label] for label in labels]\n",
    "labels = np.array(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train-Test Split\n",
    "Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why?\n",
    "Train-Test Split: To evaluate the performance of the model on unseen data.\n",
    "## Benefit\n",
    "This helps in assessing the generalizability of the model by testing it on a separate dataset that was not used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build and Train LSTM Model\n",
    "Build the LSTM model, compile it, and train it on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why?\n",
    "LSTM Model: To capture long-term dependencies in the text data.\n",
    "## Benefit\n",
    "LSTM (Long Short-Term Memory) networks are effective for sequential data, capturing context over long sequences which is crucial for understanding the dialects in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "vocab_size = 2000\n",
    "embedding_dim = 100 \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    LSTM(units=64),\n",
    "    Dense(units=len(label_mapping), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4107/4107 [==============================] - 101s 24ms/step - loss: 1.4791 - accuracy: 0.3622 - val_loss: 1.3186 - val_accuracy: 0.4577\n",
      "Epoch 2/20\n",
      "4107/4107 [==============================] - 96s 23ms/step - loss: 1.1868 - accuracy: 0.5203 - val_loss: 1.1001 - val_accuracy: 0.5668\n",
      "Epoch 3/20\n",
      "4107/4107 [==============================] - 94s 23ms/step - loss: 1.0131 - accuracy: 0.6061 - val_loss: 0.9810 - val_accuracy: 0.6168\n",
      "Epoch 4/20\n",
      "4107/4107 [==============================] - 90s 22ms/step - loss: 0.9162 - accuracy: 0.6487 - val_loss: 0.9145 - val_accuracy: 0.6522\n",
      "Epoch 5/20\n",
      "4107/4107 [==============================] - 81s 20ms/step - loss: 0.8521 - accuracy: 0.6758 - val_loss: 0.8776 - val_accuracy: 0.6618\n",
      "Epoch 6/20\n",
      "4107/4107 [==============================] - 79s 19ms/step - loss: 0.8052 - accuracy: 0.6955 - val_loss: 0.8175 - val_accuracy: 0.6930\n",
      "Epoch 7/20\n",
      "4107/4107 [==============================] - 80s 19ms/step - loss: 0.7675 - accuracy: 0.7116 - val_loss: 0.7967 - val_accuracy: 0.7038\n",
      "Epoch 8/20\n",
      "4107/4107 [==============================] - 79s 19ms/step - loss: 0.7382 - accuracy: 0.7241 - val_loss: 0.7776 - val_accuracy: 0.7096\n",
      "Epoch 9/20\n",
      "4107/4107 [==============================] - 79s 19ms/step - loss: 0.7133 - accuracy: 0.7341 - val_loss: 0.7519 - val_accuracy: 0.7207\n",
      "Epoch 10/20\n",
      "4107/4107 [==============================] - 79s 19ms/step - loss: 0.6910 - accuracy: 0.7444 - val_loss: 0.7533 - val_accuracy: 0.7185\n",
      "Epoch 11/20\n",
      "4107/4107 [==============================] - 80s 20ms/step - loss: 0.6708 - accuracy: 0.7527 - val_loss: 0.7472 - val_accuracy: 0.7240\n",
      "Epoch 12/20\n",
      "4107/4107 [==============================] - 80s 19ms/step - loss: 0.6533 - accuracy: 0.7600 - val_loss: 0.7370 - val_accuracy: 0.7330\n",
      "Epoch 13/20\n",
      "4107/4107 [==============================] - 81s 20ms/step - loss: 0.6386 - accuracy: 0.7658 - val_loss: 0.7340 - val_accuracy: 0.7335\n",
      "Epoch 14/20\n",
      "4107/4107 [==============================] - 79s 19ms/step - loss: 0.6244 - accuracy: 0.7713 - val_loss: 0.7393 - val_accuracy: 0.7309\n",
      "Epoch 15/20\n",
      "4107/4107 [==============================] - 80s 20ms/step - loss: 0.6110 - accuracy: 0.7769 - val_loss: 0.7200 - val_accuracy: 0.7389\n",
      "Epoch 16/20\n",
      "4107/4107 [==============================] - 82s 20ms/step - loss: 0.5998 - accuracy: 0.7825 - val_loss: 0.7231 - val_accuracy: 0.7377\n",
      "Epoch 17/20\n",
      "4107/4107 [==============================] - 82s 20ms/step - loss: 0.5890 - accuracy: 0.7869 - val_loss: 0.7221 - val_accuracy: 0.7415\n",
      "Epoch 18/20\n",
      "4107/4107 [==============================] - 80s 19ms/step - loss: 0.5807 - accuracy: 0.7883 - val_loss: 0.7448 - val_accuracy: 0.7366\n",
      "Epoch 19/20\n",
      "4107/4107 [==============================] - 81s 20ms/step - loss: 0.5706 - accuracy: 0.7933 - val_loss: 0.7176 - val_accuracy: 0.7439\n",
      "Epoch 20/20\n",
      "4107/4107 [==============================] - 79s 19ms/step - loss: 0.5619 - accuracy: 0.7969 - val_loss: 0.7213 - val_accuracy: 0.7391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x211b4ab9280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model.save('’Models/LSTM.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate LSTM Model\n",
    "Evaluate the LSTM model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141/1141 [==============================] - 9s 8ms/step - loss: 0.7173 - accuracy: 0.7424\n",
      "Test Loss: 0.7173075675964355\n",
      "Test Accuracy: 0.7424047589302063\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Build and Train RNN Model\n",
    "Build a Simple RNN model, compile it, and train it on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why?\n",
    "RNN Model: To compare its performance with the LSTM model.\n",
    "## Benefit\n",
    "Simple RNNs are faster and less complex than LSTMs, making them suitable for smaller datasets or simpler tasks. Comparing both models helps in selecting the most suitable architecture for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying RNN model\n",
    "from tensorflow.keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    SimpleRNN(units=64),\n",
    "    Dense(units=len(label_mapping), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4107/4107 [==============================] - 34s 8ms/step - loss: 1.5422 - accuracy: 0.3216 - val_loss: 1.5157 - val_accuracy: 0.3295\n",
      "Epoch 2/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.5016 - accuracy: 0.3412 - val_loss: 1.4687 - val_accuracy: 0.3619\n",
      "Epoch 3/20\n",
      "4107/4107 [==============================] - 34s 8ms/step - loss: 1.4044 - accuracy: 0.4066 - val_loss: 1.3576 - val_accuracy: 0.4459\n",
      "Epoch 4/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.3014 - accuracy: 0.4693 - val_loss: 1.3102 - val_accuracy: 0.4566\n",
      "Epoch 5/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.2613 - accuracy: 0.4911 - val_loss: 1.2855 - val_accuracy: 0.4825\n",
      "Epoch 6/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.2175 - accuracy: 0.5152 - val_loss: 1.1891 - val_accuracy: 0.5277\n",
      "Epoch 7/20\n",
      "4107/4107 [==============================] - 34s 8ms/step - loss: 1.1995 - accuracy: 0.5242 - val_loss: 1.3570 - val_accuracy: 0.4587\n",
      "Epoch 8/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.1964 - accuracy: 0.5248 - val_loss: 1.2415 - val_accuracy: 0.4991\n",
      "Epoch 9/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.2017 - accuracy: 0.5202 - val_loss: 1.2548 - val_accuracy: 0.4958\n",
      "Epoch 10/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.2137 - accuracy: 0.5186 - val_loss: 1.2560 - val_accuracy: 0.4973\n",
      "Epoch 11/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.2205 - accuracy: 0.5130 - val_loss: 1.3128 - val_accuracy: 0.4608\n",
      "Epoch 12/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.2239 - accuracy: 0.5091 - val_loss: 1.2753 - val_accuracy: 0.4854\n",
      "Epoch 13/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.4617 - accuracy: 0.3748 - val_loss: 1.5122 - val_accuracy: 0.3478\n",
      "Epoch 14/20\n",
      "4107/4107 [==============================] - 34s 8ms/step - loss: 1.3561 - accuracy: 0.4380 - val_loss: 1.2871 - val_accuracy: 0.4730\n",
      "Epoch 15/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.4742 - accuracy: 0.3696 - val_loss: 1.5348 - val_accuracy: 0.3263\n",
      "Epoch 16/20\n",
      "4107/4107 [==============================] - 33s 8ms/step - loss: 1.5212 - accuracy: 0.3340 - val_loss: 1.5129 - val_accuracy: 0.3315\n",
      "Epoch 17/20\n",
      "4107/4107 [==============================] - 37s 9ms/step - loss: 1.4379 - accuracy: 0.3873 - val_loss: 1.4419 - val_accuracy: 0.3761\n",
      "Epoch 18/20\n",
      "4107/4107 [==============================] - 35s 9ms/step - loss: 1.4887 - accuracy: 0.3493 - val_loss: 1.5272 - val_accuracy: 0.3331\n",
      "Epoch 19/20\n",
      "4107/4107 [==============================] - 37s 9ms/step - loss: 1.4856 - accuracy: 0.3589 - val_loss: 1.4770 - val_accuracy: 0.3551\n",
      "Epoch 20/20\n",
      "4107/4107 [==============================] - 39s 9ms/step - loss: 1.4591 - accuracy: 0.3732 - val_loss: 1.4974 - val_accuracy: 0.3413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x211bf20b760>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these steps, we have trained and evaluated LSTM and RNN models for dialect classification. We converted text data into numerical sequences, padded them, and encoded the labels. We then trained the models and evaluated their performance on test data to understand their effectiveness in classifying dialects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in conculsion we can say that the LSTM model is better than the RNN model.\n",
    "but it is still does not perform better than Logistic Regression model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
